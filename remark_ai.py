# remark_ai.py

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
from collections import Counter, defaultdict
from kiwipiepy import Kiwi
from sklearn.feature_extraction.text import TfidfVectorizer
from pathlib import Path
import os, base64, glob
from openai import OpenAI

# í•œê¸€ í°íŠ¸ ì„¤ì •
matplotlib.rc("font", family="Malgun Gothic")
plt.rcParams["axes.unicode_minus"] = False


def run_remark_analysis(input_csv_path="uploads/result.csv",
                        output_csv_path="result_merged.csv",
                        output_dir="figs"):

    # ğŸ“Œ OpenAI ì„¤ì •
    MODEL_NAME = "gpt-4o"
    MAX_OUTPUT_TOKENS = 500
    QUESTION = """
    ë‹¤ìŒì— ì œê³µë˜ëŠ” ê° ì¥ë¹„ë³„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ì—¬,
    í–¥í›„ ì´ìƒì§•í›„ ë°œìƒ ì˜ˆìƒ ì‹œì ê³¼ ìœ í˜•ì„ ì˜ˆì¸¡í•˜ê³  í‘œ í˜•íƒœë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

    [ëª©í‘œ]
    - ê° ì¥ë¹„ë³„ë¡œ í‚¤ì›Œë“œ ì‹œê³„ì—´ íŒ¨í„´ì„ ë¶„ì„
    - ìµœê·¼ ë³€ë™ë¥ , ì´ë™í‰ê· , í‘œì¤€í¸ì°¨, ê¸‰ë“±/ê¸‰ë½ ì„ê³„ì¹˜(í‰ê·  Â± 2Ïƒ) ë“± í†µê³„ì  ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ì´ìƒì§•í›„ íŒë‹¨
    - í•„ìš”í•œ ê²½ìš° ë‹¨ê¸° ì„ í˜• íšŒê·€ë‚˜ ì§€ìˆ˜í‰í™œ ë“±ì„ ì´ìš©í•´ í–¥í›„ ì¶”ì„¸ë¥¼ ì˜ˆì¸¡
    - í–¥í›„ ì–¸ì œ(ì˜ˆìƒ ë‚ ì§œ ë²”ìœ„) ì´ìƒì§•í›„ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ì§€ ì‚°ì¶œ
    - ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì´ìƒì§•í›„ ìœ í˜• ì¶”ë¡ 
    - ì´ìƒì§•í›„ì™€ ê´€ë ¨ëœ ì£¼ìš” í‚¤ì›Œë“œì™€ ê·¸ ê·¼ê±° ì œì‹œ
    - ëŒ€ì‘ ê¶Œì¥ì‚¬í•­ ì œì‹œ

    [ì¶œë ¥ í˜•ì‹]
    ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì•„ë˜ í‘œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

    | ì¥ë¹„ID | ì˜ˆì¸¡ ë‚ ì§œ | ì˜ˆìƒ ì´ìƒì§•í›„ ìœ í˜• | ê·¼ê±° í‚¤ì›Œë“œ | ëŒ€ì‘ ê¶Œì¥ì‚¬í•­ |
    
    ì¡°ê±´:
    - ë‚ ì§œëŠ” YYYY-MM-DD Â± Nì¼ í˜•ì‹ìœ¼ë¡œ ì œì‹œ
    - ê·¼ê±° í‚¤ì›Œë“œì™€ ìˆ˜ì¹˜ëŠ” ì‹¤ì œ ê·¸ë˜í”„ì—ì„œ ê´€ì°°Â·ê³„ì‚°í•œ ê°’(ë³€ë™ë¥  %, í‘œì¤€í¸ì°¨, íšŒê·€ ì¶”ì„¸ ë“±)ì„ í¬í•¨
    - ëŒ€ì‘ ê¶Œì¥ì‚¬í•­ì€ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ ì œì•ˆ
    - í‘œ ì™¸ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ
    
    """

    # ê²½ë¡œ ì„¤ì •
    BASE_DIR = Path(__file__).parent.resolve()
    in_fp = BASE_DIR / input_csv_path
    out_fp = BASE_DIR / output_csv_path
    out_dir = BASE_DIR / output_dir
    out_dir.mkdir(exist_ok=True)

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    merged = pd.read_csv(in_fp, encoding="utf-8-sig")
    kiwi = Kiwi()
    stopwords = {
        "ì˜", "ì´", "ê°€", "ì€", "ëŠ”", "ë“¤", "ì¢€", "ì˜", "ê±", "ê³¼", "ë„", "ë¥¼", "ìœ¼ë¡œ", "ì—", "í•˜ê³ ", "ë¿", "ë“±",
        "ìˆìœ¼ë©°", "ë˜ì–´", "ìˆ˜", "ìˆë‹¤", "ìˆìŒ", "ë°", "ëŒ€í•œ", "ë•Œë¬¸ì—", "ê²ƒ", "ìˆê³ ", "ìˆì–´"
    }

    # ëª…ì‚¬ ì¶”ì¶œ
    def extract_nouns(text):
        return [
            word for word, tag, _, _ in kiwi.analyze(text)[0][0]
            if tag.startswith("NN") and word not in stopwords
        ]

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    def extract_keywords_tfidf(texts, top_k=4):
        noun_texts = [" ".join(extract_nouns(text)) for text in texts]
        vectorizer = TfidfVectorizer()
        X = vectorizer.fit_transform(noun_texts)

        keywords_list = []
        for row in X:
            row_data = row.toarray().flatten()
            indices = row_data.argsort()[-top_k:][::-1]
            keywords = [vectorizer.get_feature_names_out()[i] for i in indices if row_data[i] > 0]
            keywords_list.append(", ".join(keywords))
        return keywords_list

    # í‚¤ì›Œë“œ ë¶„ì„
    merged = merged.dropna(subset=["remark", "date"]).copy()
    merged["remark_keywords"] = extract_keywords_tfidf(merged["remark"].fillna(""), top_k=4)

    # ì €ì¥
    merged.to_csv(out_fp, index=False, encoding="utf-8-sig")
    print(f"âœ… remark_keywords ì¶”ê°€ í›„ ì €ì¥ ì™„ë£Œ â†’ {out_fp.name}")

    # ì¥ë¹„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„±
    grouped = merged.groupby("equipment_id")
    for equip_id, group in grouped:
        group = group.copy()
        group["date"] = pd.to_datetime(group["date"], errors="coerce")
        group = group.dropna(subset=["date"])
        remarks = group["remark"].tolist()
        if not remarks:
            print(f"[{equip_id}] remark ì—†ìŒ â†’ ê±´ë„ˆëœ€")
            continue

        keywords = extract_keywords_tfidf(remarks, top_k=4)
        all_keywords = [kw.strip() for line in keywords for kw in line.split(",") if kw.strip()]
        counter = Counter(all_keywords)
        top_keywords = [kw for kw, _ in counter.most_common(5)]

        if not top_keywords:
            print(f"[{equip_id}] ìƒìœ„ í‚¤ì›Œë“œ ì—†ìŒ â†’ ê±´ë„ˆëœ€")
            continue

        tmp = group[["date", "remark_keywords"]].copy()
        tmp["kw_list"] = tmp["remark_keywords"].fillna("").apply(lambda s: [w.strip() for w in s.split(",") if w.strip()])
        tmp = tmp.explode("kw_list")
        tmp = tmp[tmp["kw_list"].isin(top_keywords)]
        if tmp.empty:
            print(f"[{equip_id}] ì§‘ê³„ ê²°ê³¼ ì—†ìŒ â†’ ê±´ë„ˆëœ€")
            continue

        trend_df = tmp.groupby(["date", "kw_list"]).size().unstack("kw_list").fillna(0).sort_index()
        if trend_df.to_numpy().sum() == 0:
            print(f"[{equip_id}] ëª¨ë“  ë¹ˆë„ 0 â†’ ê±´ë„ˆëœ€")
            continue

        plt.figure(figsize=(14, 6))
        for col in trend_df.columns:
            line, = plt.plot(trend_df.index, trend_df[col], marker="o", label=col)
            x_pos = trend_df.index[-1]
            y_pos = trend_df[col].iloc[-1]
            plt.text(x_pos, y_pos, f"{col}", fontsize=10, color=line.get_color(), va="center", ha="left")
        plt.title(f"Keyword Trend for Equipment {equip_id}")
        plt.xlabel("Date")
        plt.ylabel("Frequency")
        plt.xticks(rotation=45)
        plt.legend(title="Top Keywords", loc="upper left", bbox_to_anchor=(1.01, 1.0))
        plt.tight_layout()
        save_path = out_dir / f"trend_{equip_id}.png"
        plt.savefig(save_path, dpi=200, bbox_inches="tight", facecolor="white")
        plt.close()
        print(f"[{equip_id}] ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ â†’ {save_path}")

    # GPT Vision ìš”ì²­ í•¨ìˆ˜ ì •ì˜
    import os

    def ask_openai_about_images(image_paths, question, model=MODEL_NAME, max_output_tokens=MAX_OUTPUT_TOKENS):
        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            print("â— OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            return None

        client = OpenAI(api_key=api_key)
        contents = [{"type": "input_text", "text": question}]
        for path in image_paths:
            with open(path, "rb") as f:
                b64 = base64.b64encode(f.read()).decode("utf-8")
            contents.append({
                "type": "input_image",
                "image_url": f"data:image/png;base64,{b64}"
            })

        response = client.responses.create(
            model=model,
            input=[{"role": "user", "content": contents}],
            max_output_tokens=max_output_tokens,
        )
        return getattr(response, "output_text", None) or str(response)

    # GPT Vision ë¶„ì„ ìš”ì²­
    img_paths = sorted(glob.glob(str(out_dir / "trend_*.png")))
    if img_paths:
        print("\n[OpenAI] GPT ê·¸ë˜í”„ ë¶„ì„ ìš”ì²­ ì¤‘...")
        gpt_answer = ask_openai_about_images(img_paths, QUESTION)
        if gpt_answer:
            print("\n===== GPT ì‘ë‹µ =====\n")
            print(gpt_answer)
            with open(out_dir / "openai_summary.txt", "w", encoding="utf-8") as f:
                f.write(gpt_answer.strip())
            print("ğŸ“„ GPT ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ openai_summary.txt")
        else:
            print("âŒ GPT ì‘ë‹µ ì‹¤íŒ¨ ë˜ëŠ” ì‘ë‹µ ì—†ìŒ")
    else:
        print("âŒ íŠ¸ë Œë“œ ê·¸ë˜í”„ ì´ë¯¸ì§€ê°€ ì—†ì–´ GPT ë¶„ì„ ìš”ì²­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

